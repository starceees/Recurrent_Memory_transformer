{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29dcd51-7e85-4461-a19b-269586d73366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a075c306e9f4b669de8f97da05e2b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7381185aed3c45948ba9a3484edb35fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e51af3d28a47ccb7e306c93c86f187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4382c4c4cd84c2aba7420dd8860cb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load from the specific directory\n",
    "dataset = load_dataset(\"text\", \n",
    "    data_dir=\"./datasets/arxiv-dataset\",\n",
    "    data_files={\n",
    "        \"train\": \"train.txt\",\n",
    "        \"test\": \"test.txt\",\n",
    "        \"validation\": \"val.txt\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9e5c97-61e3-481a-aab8-782e009938fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94e480487ef424593dcc3e1e84c3e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"text\", \n",
    "    data_files={\n",
    "        \"train\": \"/scratch/ag8172/NLP/Project/RMT/recurrent-memory-transformer/datasets/arxiv-dataset/train.txt\",\n",
    "        \"test\": \"/scratch/ag8172/NLP/Project/RMT/recurrent-memory-transformer/datasets/arxiv-dataset/test.txt\",\n",
    "        \"validation\": \"/scratch/ag8172/NLP/Project/RMT/recurrent-memory-transformer/datasets/arxiv-dataset/val.txt\"\n",
    "    }\n",
    "    )\n",
    "\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "train_dataset = dataset[\"train\"].select(range(8500))  # First 8500 for training\n",
    "val_dataset = dataset[\"train\"].select(range(8500, 9000))  # Next 500 for validation\n",
    "test_dataset = dataset[\"test\"].select(range(1000))  # 1000 for testing (from test split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a7b225-14e2-4115-858c-635dc1126f70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few examples from training set:\n",
      "\n",
      "Dataset info:\n",
      "Number of training examples: 8500\n",
      "Features available: {'text': Value(dtype='string', id=None)}\n",
      "\n",
      "Detailed look at first example:\n",
      "{'text': '{\"article_id\": \"1506.05133\", \"article_text\": [\"arp  220 is the nearest ( @xmath3 77  mpc ) example of an ultraluminous infrared galaxy ( ulirg ) that supports star formation at extreme levels .\", \"it contains two nuclei separated by 350  pc , both surrounded by massive discs of dense molecular gas ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"radio detections of supernovae at a rate of 13 yr@xmath4 @xcite confirm that huge populations of massive stars are present with an implied star formation rate ( sfr ) of @xmath5  yr@xmath4 .\", \"although arp 220 could contain active galactic nuclei ( agns ) , particularly in the western nucleus , the observed supernova rates indicate that star formation provides a substantial fraction of the power radiated by the nuclei .\", \"the nuclei of arp  220 provide access to the high - intensity mode of star formation in dense molecular media that appears to have been more common in young galaxies .\", \"these types of environments are of special interest from a range of perspectives , including the information they can provide regarding the role of galactic winds , cosmic rays , and magnetic fields in feedback processes that influence galaxy evolution .\", \"previous investigations show that arp  220 is likely to be a hadronic cosmic ray calorimeter where all of the power in cosmic rays is absorbed within the nuclear starburst zones ( e.g. , * ? ? ?\", \"both nuclei also contain extremely intense far - infrared ( fir ) radiation fields ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"* ) , and the west nucleus is optically thick in the fir to wavelengths of @xmath6 @xcite .\", \"the production of the observed radio synchrotron emission then requires magnetic fields of milligauss strength ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"llllc physical parameters & east nucleus & west st & west cnd & references + distance & 77.0 mpc & 77.0 mpc & 77.0 mpc & + cmz radius & 70 pc & 90 pc & 30 pc & 1,2,3 + cmz disc scale height@xmath7 & 40 pc & 40 pc & 40 pc & 4 + molecular gas mass & @xmath8 @xmath9 & @xmath10 @xmath9 & @xmath8 @xmath9 & 2,5 + ionized gas mass@xmath11 & @xmath12 @xmath9 & @xmath13 @xmath9 & @xmath12 @xmath9 & + average ism density@xmath14 & @xmath157700 @xmath16 & @xmath153500 @xmath16 & @xmath1542 000 @xmath16 & + fir luminosity & @xmath17 @xmath18 & @xmath17 @xmath18 & @xmath19 @xmath18 & 2 + fir radiation field energy density@xmath20 & 40 000 ev  @xmath16 & 27 000 ev  @xmath16 & 440 000 ev  @xmath16 & + dust temperature & 90 k & 50 k & 170 k & 2,6 + sn explosion rate ( @xmath21 ) & 0.7 yr@xmath4 & 0.7 yr@xmath4 & 1.3 yr@xmath4 & 7 + star formation rate ( sfr)@xmath20 & 65 @xmath9 yr@xmath4 & 65 @xmath9 yr@xmath4 & 120 @xmath9 yr@xmath4 & + sn explosion energy@xmath22 & 10@xmath23 erg & 10@xmath23 erg & 10@xmath23 erg & + sn energy in cosmic ray protons@xmath22 & 5  20% & 5  20% & 5  20% & + ratio of primary protons to electrons ( @xmath24/@xmath25 ) & 50 & 50 & 50 & + slope of primary cosmic ray source function & 2.1  2.3 & 2.1  2.3 & 2.1  2.3 & +   +   +   +   +   +   +   +   +   +    in this paper , we study cosmic ray interactions in the arp  220 starburst nuclear regions using an updated version of the @xcite models , hereafter yegz .\", \"we develop a model with two spatial zones to accurately represent the inner and outer regions of the western nucleus as defined by its molecular gas properties @xcite .\", \"we incorporate photopion energy losses and photon  photon interactions to account for the extreme fir radiation field .\", \"we calculate the hadronic calorimetry fraction for each nucleus for the best - fitting radio models , and we predict the total @xmath2-ray and neutrino fluxes .    in section 2 , we review the physical parameters which we selected for the models .\", \"section 3 details the basic assumptions of the models and our findings for the arp  220 starburst nuclei .\", \"we present concluding remarks in section 4 .\", \"due to its extreme properties , arp 220 has been extensively studied across the electromagnetic spectrum .\", \"the nuclei of arp 220 are of particular interest as they contain more than half of the total bolometric infrared luminosity of the galaxy ( @xmath26 ; e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"* and references therein ) .\", \"as the nuclei are less than 100  pc in radius , the presence of an agn or a ` hot \\' starburst is required to explain the extraordinarily large surface brightness in the western nucleus @xcite ; however , the existence of an agn has yet to be definitively established ( e.g. , * ? ? ?\", \"further , the submillimetre observations suggest that whether or not agns are present , they are not the main heating source of the dust ( e.g. , * ? ? ?\", \"estimates of the fir luminosities of the eastern and western nuclei range from @xmath27 to @xmath28 and from @xmath29 to @xmath30 , respectively ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"the range on these luminosities is quite large due to uncertainty in the true sizes , inclinations , and opacities of the nuclei and their associated molecular disc . to keep our adopted fir luminosity in rough agreement with the observed supernova rate , we assume values of @xmath31 and @xmath32 for the eastern and western nuclei ( see table 1 ) . assuming similar ratios between the nuclei for the supernova rate and molecular gas content , we adopt values of @xmath33 yr@xmath4 , @xmath34 for the eastern nucleus and @xmath35 yr@xmath4 , @xmath36 for the western nucleus .\", \"while our assumed molecular gas masses favour conservative estimates , other estimates of the gas content suggest the masses are as high as @xmath37 @xcite .    [\", \"cols=\\\\\"<,^,^,^,^,^,^,^ \\\\\" , ]     co observations of the western nucleus imply a temperature gradient increasing towards the centre and indicate significant differences in the physical conditions between the two nuclei @xcite . @xcite model the western nucleus as two distinct dust sources  a cooler ( 50  k ) ring surrounding a hotter ( 170  k ) , dense dust core .\", \"we use this two - zone model for the western nucleus and have adjusted our single - zone model to account for the differences in temperature and density between the two regions ( see section 3 ) .\", \"for the eastern nucleus , we assume a single dust temperature of 90  k @xcite .\", \"previously , we developed and tested a model for cosmic ray interactions in the central molecular zones ( cmzs ) of star - forming and starburst galaxies ( yegz ; * ? ? ?\", \"* ; * ? ? ?\", \"our single - zone model accounts for a variety of energy losses via interactions with the interstellar medium ( ism ) , magnetic fields , and radiation fields and for energy - independent advective escape via a galactic wind ( see fig .\", \"1 ) . the resulting cosmic ray energy spectrum depends on both the total cosmic ray lifetime and a power - law injection spectrum which is directly proportional to the volume integrated supernova rate ( see yegz for further details ) .\", \"accounting for the production of secondary cosmic rays , we use our calculations of the population of energetic particles to predict the radio , @xmath2-ray , and neutrino spectra . for the @xmath2-ray spectrum\", \", we include both leptonic ( bremsstrahlung , inverse compton ) and hadronic ( neutral pion decay ) emission mechanisms . for the radio spectrum\", \", we incorporate the effects of free  free emission and absorption @xcite . as in our previous models , we assume that the ionized gas in the nuclei acts as a foreground screen that some fraction ( @xmath38 ) of the emitted synchrotron radiation passes through\", \". when the covering fraction is low ( @xmath39 ) , the radio spectrum flattens at low frequencies @xcite , and when the covering fraction is high ( @xmath40 ) , the radio spectrum turns down at low frequencies ( yegz ) .\", \"as noted above , the western nucleus in arp 220 is best modelled with two separate regions : an inner circumnuclear disc ( cnd ) with a surrounding torus ( st ) .\", \"we model the cosmic ray populations of the two regions independently , treating each region as a uniform slab .\", \"however , the effects of absorption ( free  free and @xmath2@xmath2 ) on the resulting radio and @xmath2-ray emission must be considered more carefully .\", \"absorption occurs within each emission region , and in the case of the inner cnd , absorption also occurs as the emitted radiation moves through the external , st ( see the appendix for further details ) .\", \"we perform @xmath41 tests following the approach described in yegz @xcite . comparing against radio observations for each nucleus , we vary magnetic field strength ( @xmath42 ) , wind speed ( @xmath43 ) , ionized gas density ( @xmath44 ) , and absorption fraction ( @xmath38 ) .\", \"while magnetic field strength and wind speed both directly affect the total cosmic ray lifetimes , the ionized gas density and the absorption fraction only affect the emitted radio spectrum .\", \"the free  free emission and absorption coefficients are both directly proportional to the square of @xmath44 , and so , the frequency at which the radio spectrum flattens or turns down and the amount of free  free emission at high frequencies both increase with @xmath44 .\", \"observations in @xcite , @xcite and @xcite separate the integrated fluxes of the eastern and western nuclei from the total flux , allowing us to constrain parameters for each nucleus individually . as we do not have radio observations which are separable between the two regions of the western nucleus , we can not constrain the magnetic field strength in each region separately .\", \"we therefore assume that the ratio between the magnetic field strength of the inner and outer regions of the western nucleus is equal to the square root of the ratio of the average gas densities , @xmath45 @xcite . thus , our magnetic field strength determination for the innermost western nucleus is an estimate that is guided by milky way observations .\", \"when assuming the standard 10% cosmic ray acceleration efficiency , we find that agreement between the models and the observed radio data occurs only in a very narrow area of parameter space ( see fig .\", \"the best - fitting models for the nuclei have magnetic field strengths limited to 1.0 mg for the western nucleus ( 3.5 mg in the cnd , estimated from scaling ) and 2.0  2.5 mg for the eastern nucleus ( see figs 2 and 3 and table 2 ) . as seen in fig .\", \"3 , the total radio emission in both nuclei flattens at low frequencies , and in the eastern nucleus , the radio spectrum may be turning over completely .\", \"this flattening of the radio spectra requires moderate to high absorption fractions of 50 \", \"100% in the eastern nucleus and low to moderate absorption fractions of 10  70% in the western nucleus .\", \"in addition to moderate absorption fractions in each nucleus , we also find a high contribution from thermal emission to the total radio spectrum ( see fig .\", \"3 ) , particularly in the western cnd where the majority of the radio emission is thermal above @xmath155 ghz . in part , this unusually high fraction of thermal emission is due to the inability of the model to effectively fit for free  free absorption and free  free emission simultaneously as seen in the eastern nucleus and in previous work ( see yegz ; * ? ? ?\", \"the ability of the models to accurately fit the fraction of thermal emission is further strained by the complicated nature of the western nucleus and the lack of separable radio observations .\", \"thus , in this particular case , the fractions of thermal emission in the best - fitting models have limited significance and do not necessarily contradict observations by @xcite which indicate more modest amounts of thermal emission ( @xmath46 ) .\", \"the western nucleus arp 220 has a very complex structure which we greatly simplified .\", \"while our two - zone density distribution reproduces the observed peak column density of @xmath47  @xmath48 , the mass is lower than that estimated by @xcite who derive the western nucleus gas mass from observations by the atacama large millimeter array ( alma ) of the submillimetre dust luminosity .\", \"this yields a total gas mass of @xmath49 for the western nucleus or four times our adopted value .\", \"we therefore explored the effect of increased gas mass on our model by tripling the mass in the western torus and ran a limited suite models with fixed parameters .\", \"we set the spectral index to @xmath50 and ran @xmath41 tests over the entire range of magnetic field strengths and wind speeds but over a subset of the previously tested ionized gas densities and absorption fractions .\", \"we ran tests on the western nucleus for acceleration efficiencies of 5% and 10% .    in comparing the results of these models with a larger gas mass , we find that none of the tested models are within @xmath51 of the best - fitting model at the lower assumed gas mass .\", \"further more , these results yield extremely short cosmic ray electron lifetimes such that the physical validity of the models are in question .\", \"in addition to the higher gas mass estimates , @xcite also propose a geometry where the molecular gas in both nuclei is confined to a thin ( @xmath5210  pc ) disc .\", \"the evolution of supernovae and cosmic ray interactions in this type of high molecular mass structure is beyond the scope of this study which is designed to estimate cosmic ray interaction rates in arp  220 but will need to be considered when arp  220 is detected in @xmath2-rays .      in our earlier works\", \", we demonstrated that for a given ism , the yegz models are highly sensitive to the total flux of cosmic rays ( yegz ; * ? ? ? * ) .\", \"this flux is primarily effected by the original energy input into cosmic rays and the advective time - scale , or escape fraction . the energy input into cosmic rays\", \"is determined by the supernova rate and the assumed acceleration efficiency ( @xmath53 ) . within the uncertainty in the supernova rate , we vary acceleration efficiency from 5 to 20% .    as shown above , for the standard 10% efficiency , the resulting best - fitting models are highly constrained in magnetic field strength , and we find that this is also true for an acceleration efficiency of 20% ( see table 2 ) .\", \"however , for a lower acceleration efficiency of 5% , equivalent to a lower supernova rate , we find a much larger range of acceptable fits in the eastern nucleus with magnetic field strengths ranging from 4 to 7.5 mg and wind speeds spanning our entire tested range . as such , the best - fitting models for arp 220 are essentially independent of wind ( advection ) speed ( see figs 1 and 2 ) .\", \"in contrast , a galactic wind was a vital component in modelling the cosmic ray populations of the starburst galaxies m82 and ngc 253 such that an extremely limited range of wind speeds resulted in fits within @xmath51 of the best - fitting models .\", \"the wind speed determines the advective timescale for a galaxy and the fraction of cosmic rays which escape .\", \"thus , wind speed is intrinsically tied to the proton calorimetry fraction for a galaxy which is closely related to the total radio and @xmath2-ray emission from a galaxy . other models for arp 220 have assumed fixed advection time - scales , thus ensuring proton calorimetry with the high gas densities in arp 220 @xcite . while our models agree with others in finding that the starburst regions of m82 and ngc  253 are only @xmath1540 \", \"60% proton calorimeters , we find that arp 220 s nuclei are 65  100% ( eastern ) and 90  100% ( western cnd ) proton calorimeters ( see fig .\", \"lcccccc & supernova & average gas & cosmic ray & radiation field & magnetic field & magnetic field + & power & density & energy density & energy density & energy density & strength + & ( erg  yr@xmath4 ) & ( @xmath16 ) & ( ev  @xmath16 ) & ( ev  @xmath16 ) & ( ev  @xmath16 ) & ( @xmath54 g ) + milky way & @xmath55 & 1 & 1.4 & 0.3 & 0.9 & 6 + m82 & @xmath56 & 260 & 470 & 490 & 2200 & 300 + arp 220 east & @xmath57 & 7700 & 1100 & 40 000 & @xmath58 & 6500 + arp 220 west cnd & @xmath59 & 42 000 & 2500 & 440 000 & @xmath60 & 7000 +   +      despite the uncertainty in the calorimetry fraction and the total cosmic ray flux in the eastern nucleus , we can still use our best - fitting models to make a prediction on the emitted @xmath2-ray and neutrino fluxes from arp 220 . to calculate the possible @xmath2-ray flux ,\", \"we apply the parameters of models within @xmath51 from our best - fitting radio model . combining each possible set of models from the eastern and western nucleus , we find that the resulting @xmath2-ray spectra peak around @xmath150.3 gev with a maximum flux of @xmath61 gev  @xmath62  s@xmath4 ( see fig\", \". 4 ) .    while this is roughly an order of magnitude lower than previous upper limits for arp 220 @xcite and _ fermi _ s differential sensitivity for four years of observations , it is only a factor of @xmath152 - 3 times smaller than the flux level of the recently detected ngc  2146 @xcite .\", \"we also compared our @xmath2-ray flux with the differential sensitivity 50 h of observations with the future southern cta array ( see fig .\", \"4 ) and find it to be only a factor of a few larger than our maximum flux .\", \"arp 220 may still be detectable by _ fermi _ within the next several years and is a good target for cta , especially for energies near 1  tev .\", \"in addition to making a prediction for the @xmath2-ray spectrum , we can use our same results from the radio emission to predict the neutrino flux from arp 220 .\", \"proton interactions are responsible for the creation of secondary pions , both neutral and charged . while the neutral pions decay into @xmath2-rays , the charged pions decay into a neutrino and a muon which further decays into a secondary electron or positron and two more neutrinos .\", \"the spectrum of the first neutrino from the decay of the charged pion is what we calculate here , as the calculation of the spectra of neutrinos produced during muon decay is more complex ( see * ? ? ?\", \"the flux of our maximum model is roughly @xmath63 gev  @xmath62  s@xmath4 at 0.1 pev , and at this energy , the range of possible models spans an order of magnitude in flux ( see fig .\", \"current point source sensitivity limits for the northern sky for icecube are @xmath64 gev  @xmath62  s@xmath4 , assuming a spectrum of @xmath65 @xcite .\", \"thus , it seems unlikely that arp 220 will be detected as a point source during a similar time frame by icecube .\", \"however , extreme ulirgs such as arp 220 should make a significant contribution to a diffuse neutrino background @xcite .\", \"in addition to accounting for @xmath2-ray and neutrino emission in arp 220 , we have also take into account the effects of @xmath2@xmath2 absorption due to the intense radiation fields in the nuclei @xcite . at tev energies and above\", \", @xmath2-rays and infrared photons can interact to produce an electron / positron pair @xcite .\", \"the resulting electrons will be of tev energies and most of their energy will be lost to emission of synchrotron x - rays @xcite .\", \"beginning at @xmath152  5 tev , the opacity for @xmath2@xmath2 absorption in both nuclei is significantly greater than 1 .\", \"this results in a steepening of the predicted @xmath2-ray spectrum at high energies ( see fig .\", \"we find no such increase in slope in the neutrino flux as the steepening is an effect of interactions between the @xmath2-ray and the ambient radiation field and not the cosmic ray proton population .\", \"therefore , in the case of arp 220 and other such ulirgs , the tev @xmath2-ray flux is an unreliable indicator of neutrino flux .\", \"if the effects of spectral steepening by @xmath2@xmath2 absorption are accurate , then arp 220 is unlikely to be detected by cta or other ground based cherenkov telescopes above @xmath1510 tev .\", \"in applying the yegz models to arp 220 , we find that the central starburst regions of arp 220 are moderate to complete cosmic ray proton calorimeters . as such ,\", \"the leptonic cosmic ray population is dominated by secondary electrons and positrons .\", \"the majority of these secondaries are produced at low energies ( e.g. , * ? ? ?\", \"* ) and are likely a major contributor to heating of the ism via ionization @xcite .\", \"based on our best - fitting models for the radio spectrum , we make predictions for both the @xmath2-ray and neutrino fluxes .\", \"our maximum @xmath2-ray spectrum is a factor of a 2  5 lower than previous predictions by @xcite and less than a factor of 2 lower than those by @xcite . while the predicted @xmath2-ray flux will likely be detected by _\", \"fermi _ in the future , under our model assumptions arp 220 is unlikely to be detected as a high energy neutrino point source with the current icecube observatory .\", \"additionally , @xmath2@xmath2 absorption of the tev energy @xmath2-rays make the tev @xmath2-ray flux a poor indicator of the neutrino flux in ulirgs and other such systems with extremely intense infrared radiation fields .\", \"in addition , we find that milligauss strength magnetic fields are still necessary to reproduce the observed radio fluxes from the starburst nuclei , even having assumed larger supernova rates than previous models by factors of 2  5 @xcite .\", \"differences in assumed volume and radiation field energy density across the various models account for the similar best - fitting magnetic field strengths despite the range in assumed supernova rates .\", \"the origins of milligauss strength magnetic fields in extreme starbursts and their impact on the evolution of these systems merit further examination .    while the energy density in both magnetic and radiation fields is up from starbursts like m82 by two to three orders of magnitude\", \", the change in the ratio of their energy densities is up by less than an order of magnitude ( see table 3 ) .\", \"conversely , we see a much larger change in the ratio of magnetic field energy density to cosmic ray energy density .\", \"because the cosmic ray energy density depends on the particle energy loss rate , it does not increase at the same rate as the magnetic and radiation field energy densities ( yoast - hull , gallagher , zweibel , in preparation ) .\", \"thus , the magnetic fields exceed energy equipartition with the cosmic rays by more than two orders of magnitude ( see table 3 ) .\", \"this work was supported in part by nsf ast-0907837 , nsf phy-0821899 ( to the center for magnetic self - organization in laboratory and astrophysical plasmas ) , and nsf phy-0969061 ( to the icecube collaboration ) .\", \"part of this research was carried out during jsg s appointment as a jubileumsprofessor at the chalmers university of technology .\", \"we thank susanne aalto , kazushi sakamoto , dave sanders , nick scoville , and eskil varenius for conversations on arp 220 , justin vandenbroucke and reinhard schlickeiser for discussions regarding the modelling , and francis halzen for his help and support .\", \"additionally , we thank the referee for their helpful comments .\", \"our single - zone model uses a simple solution to the radiative transfer equation of ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"* ) @xmath66 where @xmath67 is the radiative flux prior to absorption , @xmath68 is the radiative flux after absorption , and @xmath69 is the optical depth for either free - free absorption or @xmath2-@xmath2 absorption .\", \"this is still the solution for the eastern nucleus and the surrounding torus in the western nucleus . in the western cnd ,\", \"we must account for a standard emission and absorption region with an additional , external absorbing region .\", \"this observed flux is given by @xmath70 where @xmath67 is still the radiative flux prior to absorption , @xmath71 is the optical depth for @xmath2-@xmath2 or free - free absorption in the emission region , and @xmath72 is the optical depth in the external , absorbing region .\", \"aalto s. , wilner d. , spaans m. , wiedner m.  c. , sakamoto k. , black j.  h. , caldas m. , 2009 , a&a , 493 , 481 aartsen m.  g. , et al .\", \", 2014a , phys .\", \"lett . , 113 , 101101 aartsen m.  g. , et al . , 2014b , apj , 796 , 109 anantharamaiah k.  r. , viallefond f. , mohan n.  r. , goss w.  m. , zhao j.  h. , 2000 , apj , 537 , 613 barcos - muoz l. , et al . , 2015 , apj , 799 , 10 bttcher m. , harris d.  e. , krawczyski h. , 2012 , relativistic jets from active galactic nuclei .\", \"wiley , weinheim crutcher r.  m. , 2012 , ara&a , 50 , 29 dermer c.  d. , menon g. , 2009 , high energy radiation from black holes .\", \"princeton univ . press , princeton , nj downes d. , eckart a. , 2007 , a&a , 468 , l57 downes d. , solomon p.  m. , 1998 , apj , 507 , 615 draine b.  t. , 2011 , physics of the interstellar and intergalactic medium .\", \"princeton univ . press , princeton , nj ghisellini g. , 2013 , vol .\", \"873 : radiative processes in high energy astrophysics .\", \"springer - verlag , berlin lacki b.  c. , thompson t.  a. , 2013 , apj , 762 , 29 lacki b.  c. , thompson t.  a. , quataert e. , 2010b , apj , 717 , 1 lacki b.  c. , thompson t.  a. , quataert e. , loeb a. , waxman e. , 2011 , apj , 734 , 107 lisenfeld u. , vlk h.  j. , xu c. , 1996 , a&a , 314 , 745 lonsdale c.  j. , diamond p.  j. , thrall h. , smith h.  e. , lonsdale c.  j. , 2006 , apj , 647 , 185 mundell c.  g. , ferruit p. , pedlar a. , 2001 , apj , 560 , 168 murase k. , ahlers m. , lacki b.  c. , 2013 , phys .\", \"d , 88 , 121301 papadopoulos p.  p. , 2010 , apj , 720 , 226 papadopoulos p.  p. , isaak k. , van der werf p. , 2010\", \", apj , 711 , 757 papadopoulos p.  p. , thi w .- f . , miniati f. , viti s. , 2011 , mnras , 414 , 1705 parra r. , conway j.  e. , diamond p.  j. , thrall h. , lonsdale c.  j. , lonsdale c.  j. , smith h.  e. , 2007 , apj , 659 , 314 rodrguez - rico c.  a. , goss w.  m. , viallefond f. , zhao j .- h . , gomez y. , anantharamaiah k.  r. , 2005 , apj , 633 , 198 rovilos e. , diamond p.  j. , lonsdale c.  j. , smith h.  e. , lonsdale c.  j. , 2005 , mnras , 359 , 827 rybicki g.  b. , lightman a.  p. , 1979\", \", radiative processes in astrophysics .\", \"wiley , new york sakamoto k. , et al .\", \", 2008 , apj , 684 , 957 scoville n. , et al . , 2015 , apj , 800 , 70 smith h.  e. , lonsdale c.  j. , lonsdale c.  j. , diamond p.  j. , 1998 , apj , 493 , l17 soifer b.  t. , et al . , 2000 ,\", \"aj , 119 , 509 tang q .- w . , wang x .- y . , tam p .- h .\", \"t. , 2014 , apj , 794 , 26 torres d.  f. , 2004 , apj , 617 , 966 tunnard r. , et al . , 2015 , apj , 800 , 25 wilson c.  d. , rangwala n. , glenn j. , maloney p.  r. , spinoglio l. , pereira - santaella m. , 2014 , apj , 789 , l36 yoast - hull t.  m. , everett j.  e. , gallagher j.  s. , iii , zweibel e.  g. , 2013 , apj , 768 , 53 ( yegz ) yoast - hull t.  m. , gallagher j.  s. , iii , zweibel e.  g. , everett j.  e. , 2014a , apj , 780 , 137 yoast - hull t.  m. , gallagher j.  s. , iii , zweibel e.  g. , 2014b , apj , 790 , 86\"], \"abstract_text\": [\"<S> the cores of arp 220 , the closest ultraluminous infrared starburst galaxy , provide an opportunity to study interactions of cosmic rays under extreme conditions . in this paper , we model the populations of cosmic rays produced by supernovae in the central molecular zones of both starburst nuclei . </S>\", \"<S> we find that @xmath0 of cosmic rays are absorbed in these regions due to their huge molecular gas contents , and thus , the nuclei of arp 220 nearly complete proton calorimeters . </S>\", \"<S> as the cosmic ray protons collide with the interstellar medium , they produce secondary electrons that are also contained within the system and radiate synchrotron emission . using results from @xmath1 tests between the model and the observed radio spectral energy distribution </S>\", \"<S> , we predict the emergent @xmath2-ray and high - energy neutrino spectra and find the magnetic field to be at milligauss levels . because of the extremely intense far - infrared radiation fields , the @xmath2-ray spectrum steepens significantly at tev energies due to @xmath2@xmath2 absorption .    </S>\", \"<S> neutrinos  cosmic rays  galaxies : individual : arp 220  galaxies : starburst  gamma rays : galaxies  radio continuum : galaxies </S>\"], \"labels\": null, \"section_names\": [\"introduction\", \"arp 220 physical properties\", \"models & results\", \"discussion and conclusions\", \"acknowledgements\", \"two zone models\"], \"sections\": [[\"arp  220 is the nearest ( @xmath3 77  mpc ) example of an ultraluminous infrared galaxy ( ulirg ) that supports star formation at extreme levels .\", \"it contains two nuclei separated by 350  pc , both surrounded by massive discs of dense molecular gas ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"radio detections of supernovae at a rate of 13 yr@xmath4 @xcite confirm that huge populations of massive stars are present with an implied star formation rate ( sfr ) of @xmath5  yr@xmath4 .\", \"although arp 220 could contain active galactic nuclei ( agns ) , particularly in the western nucleus , the observed supernova rates indicate that star formation provides a substantial fraction of the power radiated by the nuclei .\", \"the nuclei of arp  220 provide access to the high - intensity mode of star formation in dense molecular media that appears to have been more common in young galaxies .\", \"these types of environments are of special interest from a range of perspectives , including the information they can provide regarding the role of galactic winds , cosmic rays , and magnetic fields in feedback processes that influence galaxy evolution .\", \"previous investigations show that arp  220 is likely to be a hadronic cosmic ray calorimeter where all of the power in cosmic rays is absorbed within the nuclear starburst zones ( e.g. , * ? ? ?\", \"both nuclei also contain extremely intense far - infrared ( fir ) radiation fields ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"* ) , and the west nucleus is optically thick in the fir to wavelengths of @xmath6 @xcite .\", \"the production of the observed radio synchrotron emission then requires magnetic fields of milligauss strength ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"llllc physical parameters & east nucleus & west st & west cnd & references + distance & 77.0 mpc & 77.0 mpc & 77.0 mpc & + cmz radius & 70 pc & 90 pc & 30 pc & 1,2,3 + cmz disc scale height@xmath7 & 40 pc & 40 pc & 40 pc & 4 + molecular gas mass & @xmath8 @xmath9 & @xmath10 @xmath9 & @xmath8 @xmath9 & 2,5 + ionized gas mass@xmath11 & @xmath12 @xmath9 & @xmath13 @xmath9 & @xmath12 @xmath9 & + average ism density@xmath14 & @xmath157700 @xmath16 & @xmath153500 @xmath16 & @xmath1542 000 @xmath16 & + fir luminosity & @xmath17 @xmath18 & @xmath17 @xmath18 & @xmath19 @xmath18 & 2 + fir radiation field energy density@xmath20 & 40 000 ev  @xmath16 & 27 000 ev  @xmath16 & 440 000 ev  @xmath16 & + dust temperature & 90 k & 50 k & 170 k & 2,6 + sn explosion rate ( @xmath21 ) & 0.7 yr@xmath4 & 0.7 yr@xmath4 & 1.3 yr@xmath4 & 7 + star formation rate ( sfr)@xmath20 & 65 @xmath9 yr@xmath4 & 65 @xmath9 yr@xmath4 & 120 @xmath9 yr@xmath4 & + sn explosion energy@xmath22 & 10@xmath23 erg & 10@xmath23 erg & 10@xmath23 erg & + sn energy in cosmic ray protons@xmath22 & 5  20% & 5  20% & 5  20% & + ratio of primary protons to electrons ( @xmath24/@xmath25 ) & 50 & 50 & 50 & + slope of primary cosmic ray source function & 2.1  2.3 & 2.1  2.3 & 2.1  2.3 & +   +   +   +   +   +   +   +   +   +    in this paper , we study cosmic ray interactions in the arp  220 starburst nuclear regions using an updated version of the @xcite models , hereafter yegz .\", \"we develop a model with two spatial zones to accurately represent the inner and outer regions of the western nucleus as defined by its molecular gas properties @xcite .\", \"we incorporate photopion energy losses and photon  photon interactions to account for the extreme fir radiation field .\", \"we calculate the hadronic calorimetry fraction for each nucleus for the best - fitting radio models , and we predict the total @xmath2-ray and neutrino fluxes .    in section 2 , we review the physical parameters which we selected for the models .\", \"section 3 details the basic assumptions of the models and our findings for the arp  220 starburst nuclei .\", \"we present concluding remarks in section 4 .\"], [\"due to its extreme properties , arp 220 has been extensively studied across the electromagnetic spectrum .\", \"the nuclei of arp 220 are of particular interest as they contain more than half of the total bolometric infrared luminosity of the galaxy ( @xmath26 ; e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"* and references therein ) .\", \"as the nuclei are less than 100  pc in radius , the presence of an agn or a ` hot \\' starburst is required to explain the extraordinarily large surface brightness in the western nucleus @xcite ; however , the existence of an agn has yet to be definitively established ( e.g. , * ? ? ?\", \"further , the submillimetre observations suggest that whether or not agns are present , they are not the main heating source of the dust ( e.g. , * ? ? ?\", \"estimates of the fir luminosities of the eastern and western nuclei range from @xmath27 to @xmath28 and from @xmath29 to @xmath30 , respectively ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"the range on these luminosities is quite large due to uncertainty in the true sizes , inclinations , and opacities of the nuclei and their associated molecular disc . to keep our adopted fir luminosity in rough agreement with the observed supernova rate , we assume values of @xmath31 and @xmath32 for the eastern and western nuclei ( see table 1 ) . assuming similar ratios between the nuclei for the supernova rate and molecular gas content , we adopt values of @xmath33 yr@xmath4 , @xmath34 for the eastern nucleus and @xmath35 yr@xmath4 , @xmath36 for the western nucleus .\", \"while our assumed molecular gas masses favour conservative estimates , other estimates of the gas content suggest the masses are as high as @xmath37 @xcite .    [\", \"cols=\\\\\"<,^,^,^,^,^,^,^ \\\\\" , ]     co observations of the western nucleus imply a temperature gradient increasing towards the centre and indicate significant differences in the physical conditions between the two nuclei @xcite . @xcite model the western nucleus as two distinct dust sources  a cooler ( 50  k ) ring surrounding a hotter ( 170  k ) , dense dust core .\", \"we use this two - zone model for the western nucleus and have adjusted our single - zone model to account for the differences in temperature and density between the two regions ( see section 3 ) .\", \"for the eastern nucleus , we assume a single dust temperature of 90  k @xcite .\"], [\"previously , we developed and tested a model for cosmic ray interactions in the central molecular zones ( cmzs ) of star - forming and starburst galaxies ( yegz ; * ? ? ?\", \"* ; * ? ? ?\", \"our single - zone model accounts for a variety of energy losses via interactions with the interstellar medium ( ism ) , magnetic fields , and radiation fields and for energy - independent advective escape via a galactic wind ( see fig .\", \"1 ) . the resulting cosmic ray energy spectrum depends on both the total cosmic ray lifetime and a power - law injection spectrum which is directly proportional to the volume integrated supernova rate ( see yegz for further details ) .\", \"accounting for the production of secondary cosmic rays , we use our calculations of the population of energetic particles to predict the radio , @xmath2-ray , and neutrino spectra . for the @xmath2-ray spectrum\", \", we include both leptonic ( bremsstrahlung , inverse compton ) and hadronic ( neutral pion decay ) emission mechanisms . for the radio spectrum\", \", we incorporate the effects of free  free emission and absorption @xcite . as in our previous models , we assume that the ionized gas in the nuclei acts as a foreground screen that some fraction ( @xmath38 ) of the emitted synchrotron radiation passes through\", \". when the covering fraction is low ( @xmath39 ) , the radio spectrum flattens at low frequencies @xcite , and when the covering fraction is high ( @xmath40 ) , the radio spectrum turns down at low frequencies ( yegz ) .\", \"as noted above , the western nucleus in arp 220 is best modelled with two separate regions : an inner circumnuclear disc ( cnd ) with a surrounding torus ( st ) .\", \"we model the cosmic ray populations of the two regions independently , treating each region as a uniform slab .\", \"however , the effects of absorption ( free  free and @xmath2@xmath2 ) on the resulting radio and @xmath2-ray emission must be considered more carefully .\", \"absorption occurs within each emission region , and in the case of the inner cnd , absorption also occurs as the emitted radiation moves through the external , st ( see the appendix for further details ) .\", \"we perform @xmath41 tests following the approach described in yegz @xcite . comparing against radio observations for each nucleus , we vary magnetic field strength ( @xmath42 ) , wind speed ( @xmath43 ) , ionized gas density ( @xmath44 ) , and absorption fraction ( @xmath38 ) .\", \"while magnetic field strength and wind speed both directly affect the total cosmic ray lifetimes , the ionized gas density and the absorption fraction only affect the emitted radio spectrum .\", \"the free  free emission and absorption coefficients are both directly proportional to the square of @xmath44 , and so , the frequency at which the radio spectrum flattens or turns down and the amount of free  free emission at high frequencies both increase with @xmath44 .\", \"observations in @xcite , @xcite and @xcite separate the integrated fluxes of the eastern and western nuclei from the total flux , allowing us to constrain parameters for each nucleus individually . as we do not have radio observations which are separable between the two regions of the western nucleus , we can not constrain the magnetic field strength in each region separately .\", \"we therefore assume that the ratio between the magnetic field strength of the inner and outer regions of the western nucleus is equal to the square root of the ratio of the average gas densities , @xmath45 @xcite . thus , our magnetic field strength determination for the innermost western nucleus is an estimate that is guided by milky way observations .\", \"when assuming the standard 10% cosmic ray acceleration efficiency , we find that agreement between the models and the observed radio data occurs only in a very narrow area of parameter space ( see fig .\", \"the best - fitting models for the nuclei have magnetic field strengths limited to 1.0 mg for the western nucleus ( 3.5 mg in the cnd , estimated from scaling ) and 2.0  2.5 mg for the eastern nucleus ( see figs 2 and 3 and table 2 ) . as seen in fig .\", \"3 , the total radio emission in both nuclei flattens at low frequencies , and in the eastern nucleus , the radio spectrum may be turning over completely .\", \"this flattening of the radio spectra requires moderate to high absorption fractions of 50 \", \"100% in the eastern nucleus and low to moderate absorption fractions of 10  70% in the western nucleus .\", \"in addition to moderate absorption fractions in each nucleus , we also find a high contribution from thermal emission to the total radio spectrum ( see fig .\", \"3 ) , particularly in the western cnd where the majority of the radio emission is thermal above @xmath155 ghz . in part , this unusually high fraction of thermal emission is due to the inability of the model to effectively fit for free  free absorption and free  free emission simultaneously as seen in the eastern nucleus and in previous work ( see yegz ; * ? ? ?\", \"the ability of the models to accurately fit the fraction of thermal emission is further strained by the complicated nature of the western nucleus and the lack of separable radio observations .\", \"thus , in this particular case , the fractions of thermal emission in the best - fitting models have limited significance and do not necessarily contradict observations by @xcite which indicate more modest amounts of thermal emission ( @xmath46 ) .\", \"the western nucleus arp 220 has a very complex structure which we greatly simplified .\", \"while our two - zone density distribution reproduces the observed peak column density of @xmath47  @xmath48 , the mass is lower than that estimated by @xcite who derive the western nucleus gas mass from observations by the atacama large millimeter array ( alma ) of the submillimetre dust luminosity .\", \"this yields a total gas mass of @xmath49 for the western nucleus or four times our adopted value .\", \"we therefore explored the effect of increased gas mass on our model by tripling the mass in the western torus and ran a limited suite models with fixed parameters .\", \"we set the spectral index to @xmath50 and ran @xmath41 tests over the entire range of magnetic field strengths and wind speeds but over a subset of the previously tested ionized gas densities and absorption fractions .\", \"we ran tests on the western nucleus for acceleration efficiencies of 5% and 10% .    in comparing the results of these models with a larger gas mass , we find that none of the tested models are within @xmath51 of the best - fitting model at the lower assumed gas mass .\", \"further more , these results yield extremely short cosmic ray electron lifetimes such that the physical validity of the models are in question .\", \"in addition to the higher gas mass estimates , @xcite also propose a geometry where the molecular gas in both nuclei is confined to a thin ( @xmath5210  pc ) disc .\", \"the evolution of supernovae and cosmic ray interactions in this type of high molecular mass structure is beyond the scope of this study which is designed to estimate cosmic ray interaction rates in arp  220 but will need to be considered when arp  220 is detected in @xmath2-rays .      in our earlier works\", \", we demonstrated that for a given ism , the yegz models are highly sensitive to the total flux of cosmic rays ( yegz ; * ? ? ? * ) .\", \"this flux is primarily effected by the original energy input into cosmic rays and the advective time - scale , or escape fraction . the energy input into cosmic rays\", \"is determined by the supernova rate and the assumed acceleration efficiency ( @xmath53 ) . within the uncertainty in the supernova rate , we vary acceleration efficiency from 5 to 20% .    as shown above , for the standard 10% efficiency , the resulting best - fitting models are highly constrained in magnetic field strength , and we find that this is also true for an acceleration efficiency of 20% ( see table 2 ) .\", \"however , for a lower acceleration efficiency of 5% , equivalent to a lower supernova rate , we find a much larger range of acceptable fits in the eastern nucleus with magnetic field strengths ranging from 4 to 7.5 mg and wind speeds spanning our entire tested range . as such , the best - fitting models for arp 220 are essentially independent of wind ( advection ) speed ( see figs 1 and 2 ) .\", \"in contrast , a galactic wind was a vital component in modelling the cosmic ray populations of the starburst galaxies m82 and ngc 253 such that an extremely limited range of wind speeds resulted in fits within @xmath51 of the best - fitting models .\", \"the wind speed determines the advective timescale for a galaxy and the fraction of cosmic rays which escape .\", \"thus , wind speed is intrinsically tied to the proton calorimetry fraction for a galaxy which is closely related to the total radio and @xmath2-ray emission from a galaxy . other models for arp 220 have assumed fixed advection time - scales , thus ensuring proton calorimetry with the high gas densities in arp 220 @xcite . while our models agree with others in finding that the starburst regions of m82 and ngc  253 are only @xmath1540 \", \"60% proton calorimeters , we find that arp 220 s nuclei are 65  100% ( eastern ) and 90  100% ( western cnd ) proton calorimeters ( see fig .\", \"lcccccc & supernova & average gas & cosmic ray & radiation field & magnetic field & magnetic field + & power & density & energy density & energy density & energy density & strength + & ( erg  yr@xmath4 ) & ( @xmath16 ) & ( ev  @xmath16 ) & ( ev  @xmath16 ) & ( ev  @xmath16 ) & ( @xmath54 g ) + milky way & @xmath55 & 1 & 1.4 & 0.3 & 0.9 & 6 + m82 & @xmath56 & 260 & 470 & 490 & 2200 & 300 + arp 220 east & @xmath57 & 7700 & 1100 & 40 000 & @xmath58 & 6500 + arp 220 west cnd & @xmath59 & 42 000 & 2500 & 440 000 & @xmath60 & 7000 +   +      despite the uncertainty in the calorimetry fraction and the total cosmic ray flux in the eastern nucleus , we can still use our best - fitting models to make a prediction on the emitted @xmath2-ray and neutrino fluxes from arp 220 . to calculate the possible @xmath2-ray flux ,\", \"we apply the parameters of models within @xmath51 from our best - fitting radio model . combining each possible set of models from the eastern and western nucleus , we find that the resulting @xmath2-ray spectra peak around @xmath150.3 gev with a maximum flux of @xmath61 gev  @xmath62  s@xmath4 ( see fig\", \". 4 ) .    while this is roughly an order of magnitude lower than previous upper limits for arp 220 @xcite and _ fermi _ s differential sensitivity for four years of observations , it is only a factor of @xmath152 - 3 times smaller than the flux level of the recently detected ngc  2146 @xcite .\", \"we also compared our @xmath2-ray flux with the differential sensitivity 50 h of observations with the future southern cta array ( see fig .\", \"4 ) and find it to be only a factor of a few larger than our maximum flux .\", \"arp 220 may still be detectable by _ fermi _ within the next several years and is a good target for cta , especially for energies near 1  tev .\", \"in addition to making a prediction for the @xmath2-ray spectrum , we can use our same results from the radio emission to predict the neutrino flux from arp 220 .\", \"proton interactions are responsible for the creation of secondary pions , both neutral and charged . while the neutral pions decay into @xmath2-rays , the charged pions decay into a neutrino and a muon which further decays into a secondary electron or positron and two more neutrinos .\", \"the spectrum of the first neutrino from the decay of the charged pion is what we calculate here , as the calculation of the spectra of neutrinos produced during muon decay is more complex ( see * ? ? ?\", \"the flux of our maximum model is roughly @xmath63 gev  @xmath62  s@xmath4 at 0.1 pev , and at this energy , the range of possible models spans an order of magnitude in flux ( see fig .\", \"current point source sensitivity limits for the northern sky for icecube are @xmath64 gev  @xmath62  s@xmath4 , assuming a spectrum of @xmath65 @xcite .\", \"thus , it seems unlikely that arp 220 will be detected as a point source during a similar time frame by icecube .\", \"however , extreme ulirgs such as arp 220 should make a significant contribution to a diffuse neutrino background @xcite .\", \"in addition to accounting for @xmath2-ray and neutrino emission in arp 220 , we have also take into account the effects of @xmath2@xmath2 absorption due to the intense radiation fields in the nuclei @xcite . at tev energies and above\", \", @xmath2-rays and infrared photons can interact to produce an electron / positron pair @xcite .\", \"the resulting electrons will be of tev energies and most of their energy will be lost to emission of synchrotron x - rays @xcite .\", \"beginning at @xmath152  5 tev , the opacity for @xmath2@xmath2 absorption in both nuclei is significantly greater than 1 .\", \"this results in a steepening of the predicted @xmath2-ray spectrum at high energies ( see fig .\", \"we find no such increase in slope in the neutrino flux as the steepening is an effect of interactions between the @xmath2-ray and the ambient radiation field and not the cosmic ray proton population .\", \"therefore , in the case of arp 220 and other such ulirgs , the tev @xmath2-ray flux is an unreliable indicator of neutrino flux .\", \"if the effects of spectral steepening by @xmath2@xmath2 absorption are accurate , then arp 220 is unlikely to be detected by cta or other ground based cherenkov telescopes above @xmath1510 tev .\"], [\"in applying the yegz models to arp 220 , we find that the central starburst regions of arp 220 are moderate to complete cosmic ray proton calorimeters . as such ,\", \"the leptonic cosmic ray population is dominated by secondary electrons and positrons .\", \"the majority of these secondaries are produced at low energies ( e.g. , * ? ? ?\", \"* ) and are likely a major contributor to heating of the ism via ionization @xcite .\", \"based on our best - fitting models for the radio spectrum , we make predictions for both the @xmath2-ray and neutrino fluxes .\", \"our maximum @xmath2-ray spectrum is a factor of a 2  5 lower than previous predictions by @xcite and less than a factor of 2 lower than those by @xcite . while the predicted @xmath2-ray flux will likely be detected by _\", \"fermi _ in the future , under our model assumptions arp 220 is unlikely to be detected as a high energy neutrino point source with the current icecube observatory .\", \"additionally , @xmath2@xmath2 absorption of the tev energy @xmath2-rays make the tev @xmath2-ray flux a poor indicator of the neutrino flux in ulirgs and other such systems with extremely intense infrared radiation fields .\", \"in addition , we find that milligauss strength magnetic fields are still necessary to reproduce the observed radio fluxes from the starburst nuclei , even having assumed larger supernova rates than previous models by factors of 2  5 @xcite .\", \"differences in assumed volume and radiation field energy density across the various models account for the similar best - fitting magnetic field strengths despite the range in assumed supernova rates .\", \"the origins of milligauss strength magnetic fields in extreme starbursts and their impact on the evolution of these systems merit further examination .    while the energy density in both magnetic and radiation fields is up from starbursts like m82 by two to three orders of magnitude\", \", the change in the ratio of their energy densities is up by less than an order of magnitude ( see table 3 ) .\", \"conversely , we see a much larger change in the ratio of magnetic field energy density to cosmic ray energy density .\", \"because the cosmic ray energy density depends on the particle energy loss rate , it does not increase at the same rate as the magnetic and radiation field energy densities ( yoast - hull , gallagher , zweibel , in preparation ) .\", \"thus , the magnetic fields exceed energy equipartition with the cosmic rays by more than two orders of magnitude ( see table 3 ) .\"], [\"this work was supported in part by nsf ast-0907837 , nsf phy-0821899 ( to the center for magnetic self - organization in laboratory and astrophysical plasmas ) , and nsf phy-0969061 ( to the icecube collaboration ) .\", \"part of this research was carried out during jsg s appointment as a jubileumsprofessor at the chalmers university of technology .\", \"we thank susanne aalto , kazushi sakamoto , dave sanders , nick scoville , and eskil varenius for conversations on arp 220 , justin vandenbroucke and reinhard schlickeiser for discussions regarding the modelling , and francis halzen for his help and support .\", \"additionally , we thank the referee for their helpful comments .\"], [\"our single - zone model uses a simple solution to the radiative transfer equation of ( e.g. , * ? ? ?\", \"* ; * ? ? ?\", \"* ; * ? ? ?\", \"* ) @xmath66 where @xmath67 is the radiative flux prior to absorption , @xmath68 is the radiative flux after absorption , and @xmath69 is the optical depth for either free - free absorption or @xmath2-@xmath2 absorption .\", \"this is still the solution for the eastern nucleus and the surrounding torus in the western nucleus . in the western cnd ,\", \"we must account for a standard emission and absorption region with an additional , external absorbing region .\", \"this observed flux is given by @xmath70 where @xmath67 is still the radiative flux prior to absorption , @xmath71 is the optical depth for @xmath2-@xmath2 or free - free absorption in the emission region , and @xmath72 is the optical depth in the external , absorbing region .\", \"aalto s. , wilner d. , spaans m. , wiedner m.  c. , sakamoto k. , black j.  h. , caldas m. , 2009 , a&a , 493 , 481 aartsen m.  g. , et al .\", \", 2014a , phys .\", \"lett . , 113 , 101101 aartsen m.  g. , et al . , 2014b , apj , 796 , 109 anantharamaiah k.  r. , viallefond f. , mohan n.  r. , goss w.  m. , zhao j.  h. , 2000 , apj , 537 , 613 barcos - muoz l. , et al . , 2015 , apj , 799 , 10 bttcher m. , harris d.  e. , krawczyski h. , 2012 , relativistic jets from active galactic nuclei .\", \"wiley , weinheim crutcher r.  m. , 2012 , ara&a , 50 , 29 dermer c.  d. , menon g. , 2009 , high energy radiation from black holes .\", \"princeton univ . press , princeton , nj downes d. , eckart a. , 2007 , a&a , 468 , l57 downes d. , solomon p.  m. , 1998 , apj , 507 , 615 draine b.  t. , 2011 , physics of the interstellar and intergalactic medium .\", \"princeton univ . press , princeton , nj ghisellini g. , 2013 , vol .\", \"873 : radiative processes in high energy astrophysics .\", \"springer - verlag , berlin lacki b.  c. , thompson t.  a. , 2013 , apj , 762 , 29 lacki b.  c. , thompson t.  a. , quataert e. , 2010b , apj , 717 , 1 lacki b.  c. , thompson t.  a. , quataert e. , loeb a. , waxman e. , 2011 , apj , 734 , 107 lisenfeld u. , vlk h.  j. , xu c. , 1996 , a&a , 314 , 745 lonsdale c.  j. , diamond p.  j. , thrall h. , smith h.  e. , lonsdale c.  j. , 2006 , apj , 647 , 185 mundell c.  g. , ferruit p. , pedlar a. , 2001 , apj , 560 , 168 murase k. , ahlers m. , lacki b.  c. , 2013 , phys .\", \"d , 88 , 121301 papadopoulos p.  p. , 2010 , apj , 720 , 226 papadopoulos p.  p. , isaak k. , van der werf p. , 2010\", \", apj , 711 , 757 papadopoulos p.  p. , thi w .- f . , miniati f. , viti s. , 2011 , mnras , 414 , 1705 parra r. , conway j.  e. , diamond p.  j. , thrall h. , lonsdale c.  j. , lonsdale c.  j. , smith h.  e. , 2007 , apj , 659 , 314 rodrguez - rico c.  a. , goss w.  m. , viallefond f. , zhao j .- h . , gomez y. , anantharamaiah k.  r. , 2005 , apj , 633 , 198 rovilos e. , diamond p.  j. , lonsdale c.  j. , smith h.  e. , lonsdale c.  j. , 2005 , mnras , 359 , 827 rybicki g.  b. , lightman a.  p. , 1979\", \", radiative processes in astrophysics .\", \"wiley , new york sakamoto k. , et al .\", \", 2008 , apj , 684 , 957 scoville n. , et al . , 2015 , apj , 800 , 70 smith h.  e. , lonsdale c.  j. , lonsdale c.  j. , diamond p.  j. , 1998 , apj , 493 , l17 soifer b.  t. , et al . , 2000 ,\", \"aj , 119 , 509 tang q .- w . , wang x .- y . , tam p .- h .\", \"t. , 2014 , apj , 794 , 26 torres d.  f. , 2004 , apj , 617 , 966 tunnard r. , et al . , 2015 , apj , 800 , 25 wilson c.  d. , rangwala n. , glenn j. , maloney p.  r. , spinoglio l. , pereira - santaella m. , 2014 , apj , 789 , l36 yoast - hull t.  m. , everett j.  e. , gallagher j.  s. , iii , zweibel e.  g. , 2013 , apj , 768 , 53 ( yegz ) yoast - hull t.  m. , gallagher j.  s. , iii , zweibel e.  g. , everett j.  e. , 2014a , apj , 780 , 137 yoast - hull t.  m. , gallagher j.  s. , iii , zweibel e.  g. , 2014b , apj , 790 , 86\"]]}'}\n"
     ]
    }
   ],
   "source": [
    "print(\"First few examples from training set:\")\n",
    "# print(train_dataset[:3])  # This will show first 3 examples\n",
    "\n",
    "# To get basic information about the dataset\n",
    "print(\"\\nDataset info:\")\n",
    "print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "print(f\"Features available: {train_dataset.features}\")\n",
    "\n",
    "# To see an individual example in detail\n",
    "print(\"\\nDetailed look at first example:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a024594f-8b16-4283-af37-8bf8248859b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['article_id', 'article_text', 'abstract_text', 'labels', 'section_names', 'sections'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x = json.loads(train_dataset[0]['text'])\n",
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3c0a708-e041-471a-bd17-b77b2a6586e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x['abstract_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f703c-67b7-450f-b7d4-98fac5175e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36f7e3-b2ac-4152-9434-d5f049430439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87f87658-1c55-447e-88a5-91dc45e33214",
   "metadata": {},
   "source": [
    "## Run testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3415cbba-65bf-4174-9ac8-c0e00d76cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem_size_ablation import *\n",
    "from load_saved_model_ablation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ae61d5a-07bd-407d-9d04-f6c3a3433e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"saved_models/mem32_in4096_b1_e3_1733410694/checkpoint_epoch_2.pt\"  # Adjust path as needed\n",
    "    \n",
    "# print(\"Loading model...\")\n",
    "# model, tokenizer, config, metrics = load_checkpoint(checkpoint_path)\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "840226b0-4788-4237-b021-ff971d68caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c01904-f274-4b83-b9b1-8a788909bbd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encodings\n\u001b[0;32m---> 12\u001b[0m encodings \u001b[38;5;241m=\u001b[39m prepare_dataset(\u001b[43mtokenizer\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "def prepare_dataset(tokenizer):\n",
    "    # test = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"test\")\n",
    "    # test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    # test = load_dataset(\"roneneldan/TinyStories\", split=\"validation[:5000]\")\n",
    "    test = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"test\")\n",
    "\n",
    "    encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    return encodings\n",
    "\n",
    "encodings = prepare_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4491e314-dfa5-44d6-828e-4a85eee2ab70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/280 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 3/280 [00:00<00:12, 21.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|         | 6/280 [00:00<00:11, 22.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|         | 9/280 [00:00<00:11, 23.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|         | 12/280 [00:00<00:10, 24.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|         | 15/280 [00:00<00:10, 24.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|         | 18/280 [00:00<00:10, 24.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|         | 21/280 [00:00<00:10, 24.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|         | 24/280 [00:00<00:10, 24.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|         | 27/280 [00:01<00:10, 25.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|         | 30/280 [00:01<00:09, 25.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|        | 33/280 [00:01<00:09, 25.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|        | 36/280 [00:01<00:09, 25.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|        | 39/280 [00:01<00:09, 25.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|        | 42/280 [00:01<00:09, 25.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|        | 45/280 [00:01<00:09, 25.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|        | 48/280 [00:01<00:09, 25.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|        | 51/280 [00:02<00:09, 25.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|        | 54/280 [00:02<00:08, 25.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|        | 57/280 [00:02<00:08, 25.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|       | 60/280 [00:02<00:08, 25.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|       | 63/280 [00:02<00:08, 25.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|       | 66/280 [00:02<00:08, 25.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|       | 69/280 [00:02<00:08, 25.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|       | 72/280 [00:02<00:08, 25.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|       | 75/280 [00:03<00:08, 25.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|       | 78/280 [00:03<00:08, 25.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|       | 81/280 [00:03<00:07, 25.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|       | 84/280 [00:03<00:07, 25.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|       | 87/280 [00:03<00:07, 25.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|      | 90/280 [00:03<00:07, 25.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|      | 93/280 [00:03<00:07, 25.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|      | 96/280 [00:03<00:07, 25.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|      | 99/280 [00:03<00:07, 25.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|      | 102/280 [00:04<00:07, 25.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|      | 105/280 [00:04<00:06, 25.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|      | 108/280 [00:04<00:06, 25.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|      | 111/280 [00:04<00:06, 25.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|      | 114/280 [00:04<00:06, 25.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|     | 117/280 [00:04<00:06, 25.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|     | 120/280 [00:04<00:06, 25.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|     | 123/280 [00:04<00:06, 25.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|     | 126/280 [00:05<00:06, 25.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|     | 129/280 [00:05<00:06, 25.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|     | 132/280 [00:05<00:05, 25.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|     | 135/280 [00:05<00:05, 25.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|     | 138/280 [00:05<00:05, 25.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|     | 141/280 [00:05<00:05, 25.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|    | 144/280 [00:05<00:05, 25.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|    | 147/280 [00:05<00:05, 25.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|    | 150/280 [00:05<00:05, 25.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|    | 153/280 [00:06<00:05, 25.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|    | 156/280 [00:06<00:04, 25.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|    | 159/280 [00:06<00:04, 25.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|    | 162/280 [00:06<00:04, 25.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|    | 165/280 [00:06<00:04, 25.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|    | 168/280 [00:06<00:04, 25.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|    | 171/280 [00:06<00:04, 25.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|   | 174/280 [00:06<00:04, 25.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|   | 177/280 [00:07<00:04, 25.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|   | 180/280 [00:07<00:03, 25.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|   | 183/280 [00:07<00:03, 25.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|   | 186/280 [00:07<00:03, 25.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|   | 189/280 [00:07<00:03, 25.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|   | 192/280 [00:07<00:03, 25.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|   | 195/280 [00:07<00:03, 25.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|   | 198/280 [00:07<00:03, 25.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|  | 201/280 [00:08<00:03, 25.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|  | 204/280 [00:08<00:03, 25.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|  | 207/280 [00:08<00:02, 24.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|  | 210/280 [00:08<00:02, 24.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|  | 213/280 [00:08<00:02, 25.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|  | 216/280 [00:08<00:02, 25.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|  | 219/280 [00:08<00:02, 25.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|  | 222/280 [00:08<00:02, 25.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|  | 225/280 [00:08<00:02, 25.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%| | 228/280 [00:09<00:02, 25.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%| | 231/280 [00:09<00:01, 25.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%| | 234/280 [00:09<00:01, 24.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%| | 237/280 [00:09<00:01, 24.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%| | 240/280 [00:09<00:01, 25.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%| | 243/280 [00:09<00:01, 24.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%| | 246/280 [00:09<00:01, 24.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%| | 249/280 [00:09<00:01, 25.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%| | 252/280 [00:10<00:01, 24.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%| | 255/280 [00:10<00:01, 24.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|| 258/280 [00:10<00:00, 25.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|| 261/280 [00:10<00:00, 24.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|| 264/280 [00:10<00:00, 25.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|| 267/280 [00:10<00:00, 25.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|| 270/280 [00:10<00:00, 25.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|| 273/280 [00:10<00:00, 25.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|| 276/280 [00:11<00:00, 25.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|| 279/280 [00:11<00:00, 24.96it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# def run_test(max_length,stride,\n",
    "max_length = 1024\n",
    "stride = 1024\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nll_sum = 0.0\n",
    "n_tokens = 0\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "    # print(begin_loc,end_loc,trg_len)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Accumulate the total negative log-likelihood and the total number of tokens\n",
    "    num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\n",
    "    batch_size = target_ids.size(0)\n",
    "    num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\n",
    "    nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "    n_tokens += num_loss_tokens\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
    "ppl = torch.exp(avg_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1487db6a-4ea1-4f1a-ae93-7f0fe7a2b493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.1627, device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e07c48ea-8921-4aef-aaf4-53a7a94253a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.1627, device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4359d342-0372-4688-9fc7-c04f8f766bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_perplexity_test(encodings, max_length=1024, stride=1024, n_chunks=4):\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "    \n",
    "    # Process text in chunks of max_length\n",
    "    for chunk in range(n_chunks):\n",
    "        chunk_start = chunk * max_length\n",
    "        chunk_end = min((chunk + 1) * max_length, seq_len)\n",
    "        \n",
    "        prev_end_loc = chunk_start\n",
    "        # Process each chunk with sliding windows\n",
    "        for begin_loc in range(chunk_start, chunk_end, stride):\n",
    "            end_loc = min(begin_loc + max_length, chunk_end)\n",
    "            trg_len = end_loc - prev_end_loc\n",
    "            \n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss\n",
    "                \n",
    "            num_valid_tokens = (target_ids != -100).sum().item()\n",
    "            batch_size = target_ids.size(0)\n",
    "            num_loss_tokens = num_valid_tokens - batch_size\n",
    "            \n",
    "            nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "            n_tokens += num_loss_tokens\n",
    "            prev_end_loc = end_loc\n",
    "            \n",
    "    avg_nll = nll_sum / n_tokens\n",
    "    ppl = torch.exp(avg_nll)\n",
    "    return ppl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ddee2a8-ad8b-4e22-88cf-127140add91c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [94,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [11,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1308: indexSelectLargeIndex: block: [47,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppl_1024_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mrun_perplexity_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ppl_1024_chunks\n",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m, in \u001b[0;36mrun_perplexity_test\u001b[0;34m(encodings, max_length, stride, n_chunks)\u001b[0m\n\u001b[1;32m     19\u001b[0m target_ids[:, :\u001b[38;5;241m-\u001b[39mtrg_len] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     neg_log_likelihood \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     25\u001b[0m num_valid_tokens \u001b[38;5;241m=\u001b[39m (target_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1271\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1271\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1031\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[0;32m-> 1031\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# Attention mask.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/ag8172/conda-envs/nlp-project/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "ppl_1024_chunks = run_perplexity_test(encodings,max_length=1024, stride=1024, n_chunks=4)\n",
    "ppl_1024_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb44371-96cb-4424-9cec-5b9372ec2914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(37.9856, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl_4096 = run_perplexity_test(encodings, max_length=4096, stride=4096, n_chunks=1)\n",
    "ppl_4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99f8ad-ee6d-4756-99d3-14877c142be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78ce81b9-f843-4946-af50-0f895bc4e4d2",
   "metadata": {},
   "source": [
    "## Run a Script for all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94e12f2-bee3-452a-8dac-18a0812d2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def prepare_dataset(tokenizer):\n",
    "    # test = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"test\")\n",
    "    test = test = load_dataset(\"roneneldan/TinyStories\", split=\"validation[:5000]\")\n",
    "    encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec872f0-a554-4d6d-b531-65c578387f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_perplexity_test(encodings, max_length=1024, stride=1024, n_chunks=4):\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "    \n",
    "    # Process text in chunks of max_length\n",
    "    for chunk in range(n_chunks):\n",
    "        chunk_start = chunk * max_length\n",
    "        chunk_end = min((chunk + 1) * max_length, seq_len)\n",
    "        \n",
    "        prev_end_loc = chunk_start\n",
    "        # Process each chunk with sliding windows\n",
    "        for begin_loc in range(chunk_start, chunk_end, stride):\n",
    "            end_loc = min(begin_loc + max_length, chunk_end)\n",
    "            trg_len = end_loc - prev_end_loc\n",
    "            \n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss\n",
    "                \n",
    "            num_valid_tokens = (target_ids != -100).sum().item()\n",
    "            batch_size = target_ids.size(0)\n",
    "            num_loss_tokens = num_valid_tokens - batch_size\n",
    "            \n",
    "            nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "            n_tokens += num_loss_tokens\n",
    "            prev_end_loc = end_loc\n",
    "            \n",
    "    avg_nll = nll_sum / n_tokens\n",
    "    ppl = torch.exp(avg_nll)\n",
    "    return ppl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f523fcb5-9e37-4738-a398-2d167ff78a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mem2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ag8172/NLP/Project/RMT/recurrent-memory-transformer/load_saved_model_ablation.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with config: {'input_size': 4096, 'memory_size': 2, 'batch_size': 1, 'num_epochs': 3, 'model_name': 'gpt2', 'block_size': 1020, 'n_segments': 5, 'history_size': 4080, 'learning_rate': 0.0001, 'train_steps': 100, 'eval_steps': 100, 'run_name': 'mem2_in4096_b1_e3_1733427878', 'save_dir': PosixPath('saved_models/mem2_in4096_b1_e3_1733427878')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|| 245/246 [00:43<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window PPL: 45.756961822509766\n",
      "--------------------\n",
      "Model mem4\n",
      "Running with config: {'input_size': 4096, 'memory_size': 4, 'batch_size': 1, 'num_epochs': 3, 'model_name': 'gpt2', 'block_size': 1016, 'n_segments': 5, 'history_size': 4064, 'learning_rate': 0.0001, 'train_steps': 100, 'eval_steps': 100, 'run_name': 'mem4_in4096_b1_e3_1733427934', 'save_dir': PosixPath('saved_models/mem4_in4096_b1_e3_1733427934')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|| 245/246 [00:43<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window PPL: 52.18146514892578\n",
      "--------------------\n",
      "Model mem8\n",
      "Running with config: {'input_size': 4096, 'memory_size': 8, 'batch_size': 1, 'num_epochs': 3, 'model_name': 'gpt2', 'block_size': 1008, 'n_segments': 5, 'history_size': 4032, 'learning_rate': 0.0001, 'train_steps': 100, 'eval_steps': 100, 'run_name': 'mem8_in4096_b1_e3_1733427989', 'save_dir': PosixPath('saved_models/mem8_in4096_b1_e3_1733427989')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|| 245/246 [00:43<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window PPL: 52.6707649230957\n",
      "--------------------\n",
      "Model mem16\n",
      "Running with config: {'input_size': 4096, 'memory_size': 16, 'batch_size': 1, 'num_epochs': 3, 'model_name': 'gpt2', 'block_size': 992, 'n_segments': 5, 'history_size': 3968, 'learning_rate': 0.0001, 'train_steps': 100, 'eval_steps': 100, 'run_name': 'mem16_in4096_b1_e3_1733428045', 'save_dir': PosixPath('saved_models/mem16_in4096_b1_e3_1733428045')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|| 245/246 [00:44<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window PPL: 45.806922912597656\n",
      "--------------------\n",
      "Model mem32\n",
      "Running with config: {'input_size': 4096, 'memory_size': 32, 'batch_size': 1, 'num_epochs': 3, 'model_name': 'gpt2', 'block_size': 960, 'n_segments': 5, 'history_size': 3840, 'learning_rate': 0.0001, 'train_steps': 100, 'eval_steps': 100, 'run_name': 'mem32_in4096_b1_e3_1733428101', 'save_dir': PosixPath('saved_models/mem32_in4096_b1_e3_1733428101')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|| 245/246 [00:45<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window PPL: 49.698638916015625\n",
      "--------------------\n",
      "Model mem64\n",
      "Running with config: {'input_size': 4096, 'memory_size': 64, 'batch_size': 1, 'num_epochs': 3, 'model_name': 'gpt2', 'block_size': 896, 'n_segments': 5, 'history_size': 3584, 'learning_rate': 0.0001, 'train_steps': 100, 'eval_steps': 100, 'run_name': 'mem64_in4096_b1_e3_1733428158', 'save_dir': PosixPath('saved_models/mem64_in4096_b1_e3_1733428158')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|| 245/246 [00:47<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window PPL: 55.40236282348633\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "runs = [\n",
    "\"mem2_in4096_b1_e3_1733388505\",\n",
    "\"mem4_in4096_b1_e3_1733394016\",\n",
    "\"mem8_in4096_b1_e3_1733399520\",\n",
    "\"mem16_in4096_b1_e3_1733405065\",\n",
    "\"mem32_in4096_b1_e3_1733410694\",\n",
    "\"mem64_in4096_b1_e3_1733416296\",\n",
    "]\n",
    "\n",
    "for run in runs:\n",
    "    checkpoint_path = f\"saved_models/{run}/checkpoint_epoch_2.pt\"  # Adjust path as needed\n",
    "        \n",
    "    print(f\"Model {run.split('_')[0]}\")\n",
    "    model, tokenizer, config, metrics = load_checkpoint(checkpoint_path)\n",
    "    encodings = prepare_dataset(tokenizer)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # def run_test(max_length,stride,\n",
    "    max_length = 4096\n",
    "    stride = 4096\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    \n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        # print(begin_loc,end_loc,trg_len)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "    \n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "    \n",
    "        # Accumulate the total negative log-likelihood and the total number of tokens\n",
    "        num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\n",
    "        batch_size = target_ids.size(0)\n",
    "        num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\n",
    "        nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "        n_tokens += num_loss_tokens\n",
    "    \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
    "    ppl = torch.exp(avg_nll)\n",
    "        \n",
    "    # ppl_4096 = run_perplexity_test(encodings, max_length=4096, stride=4096, n_chunks=1)\n",
    "    \n",
    "    print(f\"Context Window PPL: {ppl}\")\n",
    "    # print(f\"Full PPL: {ppl_4096}\")\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f280b678-2e84-49a6-9234-7b5faf17c61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 280/281 [00:11<00:00, 25.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.9405, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_perplexity(model, encodings, model_max_length, device):\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "    prev_end_loc = 0\n",
    "\n",
    "    # If sequence length is greater than model's max length, process in chunks\n",
    "    if seq_len > model_max_length:\n",
    "        print('here')\n",
    "        stride = model_max_length  # No overlap when processing long sequences\n",
    "        \n",
    "        # Process in chunks of model_max_length\n",
    "        for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "            end_loc = min(begin_loc + model_max_length, seq_len)\n",
    "            trg_len = end_loc - prev_end_loc\n",
    "            \n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss\n",
    "            \n",
    "            num_valid_tokens = (target_ids != -100).sum().item()\n",
    "            batch_size = target_ids.size(0)\n",
    "            num_loss_tokens = num_valid_tokens - batch_size\n",
    "            \n",
    "            nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "            n_tokens += num_loss_tokens\n",
    "            prev_end_loc = end_loc\n",
    "            \n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "    else:\n",
    "        # If sequence fits in model's context window, process it all at once\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "        \n",
    "        num_valid_tokens = (target_ids != -100).sum().item()\n",
    "        batch_size = target_ids.size(0)\n",
    "        num_loss_tokens = num_valid_tokens - batch_size\n",
    "        \n",
    "        nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "        n_tokens += num_loss_tokens\n",
    "\n",
    "    avg_nll = nll_sum / n_tokens\n",
    "    ppl = torch.exp(avg_nll)\n",
    "    return ppl\n",
    "\n",
    "# Usage example:\n",
    "# For 1024 model\n",
    "ppl_1024 = calculate_perplexity(model, encodings, model_max_length=1024, device=device)\n",
    "print(ppl_1024)\n",
    "# For 4096 model\n",
    "# ppl_4096 = calculate_perplexity(model_4096, encodings, model_max_length=4096, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8644a6da-0bd7-4765-9895-15c3f608c934",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_4096' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ppl_4096 \u001b[38;5;241m=\u001b[39m calculate_perplexity(\u001b[43mmodel_4096\u001b[49m, encodings, model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_4096' is not defined"
     ]
    }
   ],
   "source": [
    "ppl_4096 = calculate_perplexity(model_4096, encodings, model_max_length=1024, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e3028c2-a3bc-46c2-83fb-427a921986df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262144, 1048576)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 * 256 *256, 1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f328ab-2b46-4c43-9376-d868cc186865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687bb1c8-68cc-47a5-9a32-4be843a690b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28cfb0a-1c4f-40e5-a77a-b0cc0cdfc871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e54d512d-f4ff-44f2-8b14-81285cfb5efb",
   "metadata": {},
   "source": [
    "## Run Older Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f5f1f-f13e-4c06-a4fd-134c9b6ebef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aaca848-05cd-47da-b14e-9e6203f074ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def prepare_dataset(tokenizer):\n",
    "    test = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"test\")\n",
    "    # test = test = load_dataset(\"roneneldan/TinyStories\", split=\"validation[:5000]\")\n",
    "    # test = load_dataset(\"pg19\", split=\"test[:100]\")\n",
    "    # test = test.select(range(200))\n",
    "    # test = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"test\")\n",
    "    # test = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")\n",
    "    \n",
    "    encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daba7e91-4de1-4254-b3dd-5e3dea5e8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from modeling_rmt.language_modeling import MemoryCell, RecurrentWrapper\n",
    "import argparse\n",
    "\n",
    "def load_rmt_model(model_path, args=None):\n",
    "    \"\"\"\n",
    "    Load a saved RMT model and its tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model (.bin file)\n",
    "        args (argparse.Namespace, optional): Arguments containing model configuration.\n",
    "                                          If None, default values will be used.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (loaded_model, tokenizer)\n",
    "    \"\"\"\n",
    "    # Default arguments if none provided\n",
    "    if args is None:\n",
    "        class DefaultArgs:\n",
    "            def __init__(self):\n",
    "                self.model_name = 'gpt2'\n",
    "                self.memory_size = 4\n",
    "                self.input_size = 1024\n",
    "                self.n_segments = 2\n",
    "                self.lora_r = 16\n",
    "                self.lora_alpha = 32\n",
    "                self.lora_dropout = 0.1\n",
    "        args = DefaultArgs()\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Initialize base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=[\"c_attn\"],\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # Prepare model with LoRA\n",
    "    model = prepare_model_for_kbit_training(base_model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Set up RMT wrapper\n",
    "    block_size = 1024 - 2 * args.memory_size\n",
    "    cell = MemoryCell(model, num_mem_tokens=args.memory_size)\n",
    "    rmt_model = RecurrentWrapper(cell,\n",
    "                               segment_size=block_size,\n",
    "                               max_n_segments=args.n_segments)\n",
    "    \n",
    "    # Load saved weights\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    rmt_model.load_state_dict(state_dict)\n",
    "    rmt_model = rmt_model.to(device)\n",
    "    \n",
    "    return rmt_model, tokenizer\n",
    "\n",
    "def load_rmt_model_lora(model_path,mem_size, config=None):\n",
    "    \"\"\"\n",
    "    Load a saved RMT model from a .bin file\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model .bin file\n",
    "        config (dict, optional): Configuration dictionary containing model parameters.\n",
    "            If None, will attempt to parse from the model filename.\n",
    "            Should include: memory_size, input_size\n",
    "            \n",
    "    Returns:\n",
    "        model: Loaded RecurrentWrapper model\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        # Try to parse config from filename\n",
    "        # Expected format: rmt_modelname_inputsize_memsize.bin\n",
    "        try:\n",
    "            filename = os.path.basename(model_path)\n",
    "            parts = filename.replace('.bin', '').split('_')\n",
    "            config = {\n",
    "                'model_name': 'gpt2',\n",
    "                'input_size': 4096,\n",
    "                'memory_size': mem_size,\n",
    "                'lora_r': 16,\n",
    "                'lora_alpha': 32,\n",
    "                'lora_dropout': 0.1\n",
    "            }\n",
    "        except:\n",
    "            raise ValueError(\"Could not parse config from filename. Please provide config dictionary.\")\n",
    "\n",
    "    # Convert config to args namespace to match training setup\n",
    "    args = argparse.Namespace(**config)\n",
    "    \n",
    "    # Setup model architecture\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Setup LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_r,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=[\"c_attn\"],\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # Prepare model with LoRA\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "    base_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    # Calculate segments and sizes\n",
    "    block_size = 1024 - 2 * args.memory_size\n",
    "    args.n_segments = math.ceil(args.input_size / block_size)\n",
    "    args.history_size = (args.n_segments - 1) * block_size\n",
    "    \n",
    "    # Create RMT model\n",
    "    cell = MemoryCell(base_model, num_mem_tokens=args.memory_size)\n",
    "    model = RecurrentWrapper(cell,\n",
    "                           segment_size=block_size,\n",
    "                           max_n_segments=args.n_segments)\n",
    "    \n",
    "    # Load saved weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to('cuda')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e804bd7-e0e0-49a7-bb5a-4033647f86f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = f\"results/run_jnvnwhoq/model.bin\"  # Adjust path as needed\n",
    "        \n",
    "# print(f\"Model jnvnwhoq\")\n",
    "# model, tokenizer = load_rmt_model_lora(checkpoint_path,64)\n",
    "# encodings = prepare_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3134ef3-a233-4722-b62d-fa116964106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54559517/ipykernel_485046/1110075982.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (297300 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window PPL: 37.36375427246094\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "runs = [\n",
    "\"run_lvmpgbkg\",\n",
    "\"run_0llr7i14\",\n",
    "\"run_45a0rxa6\",\n",
    "]\n",
    "\n",
    "runs = [\n",
    "[\"run_hwr85mqs\",128],\n",
    "# [\"run_jnvnwhoq\",64],\n",
    "# [\"run_fla0gr8g\",32],\n",
    "# [\"run_1vglsl0v\",16],\n",
    "# [\"run_x1rqyscx\",8],\n",
    "# [\"run_f6ez630v\",4],\n",
    "# [\"run_wneyhtc1\",2]\n",
    "]\n",
    "\n",
    "for run in runs:\n",
    "    checkpoint_path = f\"results/{run[0]}/model.bin\"  # Adjust path as needed\n",
    "        \n",
    "    print(f\"Model {run[0].split('_')[0]}\")\n",
    "    # model, tokenizer = load_rmt_model(checkpoint_path)\n",
    "    model, tokenizer = load_rmt_model_lora(checkpoint_path,run[1])\n",
    "    encodings = prepare_dataset(tokenizer)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # def run_test(max_length,stride,\n",
    "    # max_length = 1024\n",
    "    # stride = 1024\n",
    "\n",
    "    # lora memsize models\n",
    "    max_length = 4096\n",
    "    stride = 4096\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    \n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        # print(begin_loc,end_loc,trg_len)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "    \n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "    \n",
    "        # Accumulate the total negative log-likelihood and the total number of tokens\n",
    "        num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\n",
    "        batch_size = target_ids.size(0)\n",
    "        num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\n",
    "        nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "        n_tokens += num_loss_tokens\n",
    "    \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
    "    ppl = torch.exp(avg_nll)\n",
    "    \n",
    "    # ppl_4096 = run_perplexity_test(encodings, max_length=1024, stride=1024, n_chunks=1)\n",
    "    \n",
    "    print(f\"Context Window PPL: {ppl}\")\n",
    "    # print(f\"Full PPL: {ppl_4096}\")\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182d9af-2b26-40ff-a959-96ff5e2579b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d8eb2-5586-45db-be3c-b9d16d087328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e885f-d31d-42b1-ade8-8a381272fbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f562dd-e883-4599-be98-682b2a8cb42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3129bd6e-4cd6-494c-8982-56ed138d9adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ag8172/NLP/Project/RMT/recurrent-memory-transformer/load_saved_model_ablation.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with config: {'input_size': 4096, 'memory_size': 32, 'batch_size': 1, 'num_epochs': 3, 'model_name': 'gpt2', 'block_size': 960, 'n_segments': 5, 'history_size': 3840, 'learning_rate': 0.0001, 'train_steps': 100, 'eval_steps': 100, 'run_name': 'mem32_in4096_b1_e3_1733437824', 'save_dir': PosixPath('saved_models/mem32_in4096_b1_e3_1733437824')}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"saved_models/mem32_in4096_b1_e3_1733410694/checkpoint_epoch_2.pt\"  # Adjust path as needed\n",
    "    \n",
    "print(\"Loading model...\")\n",
    "model_4096, tokenizer_4096, config, metrics = load_checkpoint(checkpoint_path)\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "model_1024 = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "tokenizer_1024 = GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64b08cc-c04b-476c-9ff3-66be1944abc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "def prepare_dataset(tokenizer):\n",
    "    # test = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"test\")\n",
    "    test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    # test = load_dataset(\"roneneldan/TinyStories\", split=\"validation[:5000]\")\n",
    "    encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    return encodings\n",
    "\n",
    "encodings_4096 = prepare_dataset(tokenizer_4096)\n",
    "encodings_1024 = prepare_dataset(tokenizer_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31d2c25-b6ec-4b95-9212-6af44901048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_perplexity(model, encodings, max_length, stride, device):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a given model and encodings, processing in chunks.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        encodings: The encoded text\n",
    "        max_length: Maximum sequence length the model can handle\n",
    "        stride: How much to shift the window by (usually equal to max_length for non-overlapping chunks)\n",
    "        device: The device to run the model on\n",
    "    \"\"\"\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "    prev_end_loc = 0\n",
    "    \n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        \n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            \n",
    "        num_valid_tokens = (target_ids != -100).sum().item()\n",
    "        batch_size = target_ids.size(0)\n",
    "        num_loss_tokens = num_valid_tokens - batch_size\n",
    "        \n",
    "        nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "        n_tokens += num_loss_tokens\n",
    "        prev_end_loc = end_loc\n",
    "        \n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "            \n",
    "    avg_nll = nll_sum / n_tokens\n",
    "    ppl = torch.exp(avg_nll)\n",
    "    \n",
    "    return ppl, avg_nll, n_tokens\n",
    "\n",
    "# Example usage for model with 1024 context window\n",
    "def evaluate_1024_model(model_1024, encodings, device):\n",
    "    \"\"\"\n",
    "    Evaluate model with 1024 context window on 4096 sequence\n",
    "    \"\"\"\n",
    "    ppl, avg_nll, n_tokens = calculate_perplexity(\n",
    "        model=model_1024,\n",
    "        encodings=encodings,\n",
    "        max_length=1024,\n",
    "        stride=1024,  # non-overlapping chunks\n",
    "        device=device\n",
    "    )\n",
    "    return {\n",
    "        'perplexity': ppl,\n",
    "        'avg_nll': avg_nll,\n",
    "        'tokens_processed': n_tokens\n",
    "    }\n",
    "\n",
    "# Example usage for model with 4096 context window\n",
    "def evaluate_4096_model(model_4096, encodings, device):\n",
    "    \"\"\"\n",
    "    Evaluate model with 4096 context window on same sequence\n",
    "    \"\"\"\n",
    "    ppl, avg_nll, n_tokens = calculate_perplexity(\n",
    "        model=model_4096,\n",
    "        encodings=encodings,\n",
    "        max_length=4096,\n",
    "        stride=4096,  # process entire sequence at once\n",
    "        device=device\n",
    "    )\n",
    "    return {\n",
    "        'perplexity': ppl,\n",
    "        'avg_nll': avg_nll,\n",
    "        'tokens_processed': n_tokens\n",
    "    }\n",
    "\n",
    "# Compare both models\n",
    "def compare_models(model_1024, model_4096, encodings_1024, encodings_4096, device):\n",
    "    \"\"\"\n",
    "    Compare perplexity between models with different context windows\n",
    "    \"\"\"\n",
    "    results_1024 = evaluate_1024_model(model_1024, encodings, device)\n",
    "    results_4096 = evaluate_4096_model(model_4096, encodings, device)\n",
    "    \n",
    "    print(\"1024 Context Window Results:\")\n",
    "    print(f\"Perplexity: {results_1024['perplexity']:.2f}\")\n",
    "    print(f\"Average NLL: {results_1024['avg_nll']:.4f}\")\n",
    "    print(f\"Tokens processed: {results_1024['tokens_processed']}\")\n",
    "    print(\"\\n4096 Context Window Results:\")\n",
    "    print(f\"Perplexity: {results_4096['perplexity']:.2f}\")\n",
    "    print(f\"Average NLL: {results_4096['avg_nll']:.4f}\")\n",
    "    print(f\"Tokens processed: {results_4096['tokens_processed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0680816-0587-4c79-b075-4b6f62c45e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 280/281 [00:11<00:00, 23.91it/s]\n",
      " 99%|| 70/71 [00:12<00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 Context Window Results:\n",
      "Perplexity: 29.94\n",
      "Average NLL: 3.3992\n",
      "Tokens processed: 287363\n",
      "\n",
      "4096 Context Window Results:\n",
      "Perplexity: 35.71\n",
      "Average NLL: 3.5755\n",
      "Tokens processed: 287573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_models(model_1024, model_4096, encodings_1024, encodings_4096, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a174a5-8ed9-4cda-abce-01f2c9ec7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "so i guess we can say that 4096 rmt perp isnt so bad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-project)",
   "language": "python",
   "name": "nlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
